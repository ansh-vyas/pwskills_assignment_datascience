{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7553bf64",
   "metadata": {},
   "source": [
    "## Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d72bf0",
   "metadata": {},
   "source": [
    "Eigenvalues and eigenvectors are mathematical concepts used in linear algebra, and they are closely related to the eigen-decomposition approach, which is used in various matrix and data analysis techniques, including Principal Component Analysis (PCA).\n",
    "\n",
    "**Eigenvalues:**\n",
    "Eigenvalues are scalar values that represent how a linear transformation (a matrix) stretches or compresses space along its principal axes. In the context of a square matrix, like a covariance matrix, the eigenvalues represent the scaling factors by which the corresponding eigenvectors are stretched or compressed.\n",
    "\n",
    "**Eigenvectors:**\n",
    "Eigenvectors are non-zero vectors that remain in the same direction after the application of a linear transformation represented by a matrix. In the context of a covariance matrix, eigenvectors represent the principal directions or axes along which the data has maximum variance.\n",
    "\n",
    "**Eigen-Decomposition:**\n",
    "Eigen-decomposition is a mathematical procedure used to break down a square matrix into its constituent eigenvalues and eigenvectors. It is typically represented as follows:\n",
    "\n",
    "<br>Av =  λv\n",
    "<br>AQ = Q Λ\n",
    "<br>A = Q Λ Q⁻¹\n",
    "\n",
    "Where:\n",
    "- A is the square matrix to be decomposed.\n",
    "- Q is a matrix whose columns are the eigenvectors of A.\n",
    "- Λ (Lambda) is a diagonal matrix containing the eigenvalues of A.\n",
    "- Q⁻¹ is the inverse of the matrix Q.\n",
    "\n",
    "The eigenvalues in Λ represent how the matrix A scales space in different directions, and the eigenvectors in Q represent the principal directions along which A operates.\n",
    "\n",
    "**Example:**\n",
    "Let's consider a simple 2x2 square matrix A:\n",
    "\n",
    "A = | 3  1 |\n",
    "   <br>&nbsp; &nbsp; &nbsp;&nbsp;   | 1  2 |\n",
    "\n",
    "To find the eigenvalues and eigenvectors of A, we solve the following equation:\n",
    "\n",
    "A * v = λ * v\n",
    "\n",
    "Where:\n",
    "- A is the matrix.\n",
    "- v is the eigenvector.\n",
    "- λ (lambda) is the eigenvalue.\n",
    "\n",
    "Solving this equation for eigenvalues and eigenvectors yields the following results:\n",
    "\n",
    "Eigenvalues (λ):\n",
    "- λ₁ = 4\n",
    "- λ₂ = 1\n",
    "\n",
    "Eigenvectors (v):\n",
    "- For λ₁ = 4, the eigenvector v₁ is [1, 1].\n",
    "- For λ₂ = 1, the eigenvector v₂ is [-1, 1].\n",
    "\n",
    "So, in this example, we have found two eigenvalues and their corresponding eigenvectors. The eigenvalues represent how the matrix A scales space in different directions, and the eigenvectors represent the principal directions along which A operates.\n",
    "\n",
    "In the context of PCA, eigen-decomposition of the covariance matrix of data allows us to identify these principal directions (eigenvectors) and quantify the amount of variance explained by each direction (eigenvalues), which is fundamental to dimensionality reduction and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eeec0",
   "metadata": {},
   "source": [
    "## Q2. What is eigen decomposition and what is its significance in linear algebra?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6974e6d7",
   "metadata": {},
   "source": [
    "Eigen-decomposition, also known as spectral decomposition or eigendecomposition, is a fundamental concept in linear algebra. It involves breaking down a square matrix into a specific set of matrices and vectors. This decomposition is significant in various mathematical and practical contexts, including machine learning, data analysis, physics, and engineering. Here's what eigen-decomposition entails and its significance:\n",
    "\n",
    "**Eigen-Decomposition:**\n",
    "Eigen-decomposition decomposes a square matrix A into the following form:\n",
    "\n",
    "A = Q Λ Q⁻¹\n",
    "\n",
    "Where:\n",
    "- A is the original square matrix (often a real or complex symmetric matrix).\n",
    "- Q is a matrix whose columns are the eigenvectors of A.\n",
    "- Λ (Lambda) is a diagonal matrix containing the eigenvalues of A.\n",
    "- Q⁻¹ is the inverse of matrix Q.\n",
    "\n",
    "**Significance of Eigen-Decomposition:**\n",
    "1. Helps understand matrix properties and behavior.\n",
    "2. Simplifies solving linear systems.\n",
    "3. Plays a key role in Principal Component Analysis (PCA).\n",
    "4. Is fundamental in quantum mechanics, structural analysis, and other fields.\n",
    "5. Aids in analyzing Markov chains and signal processing.\n",
    "6. Enables efficient diagonalization and spectral decomposition of matrices.\n",
    "7. Has applications in quantum computing and various scientific domains.\n",
    "\n",
    "In summary, eigen-decomposition is a powerful mathematical tool that breaks down a matrix into its constituent eigenvalues and eigenvectors. Its significance spans multiple disciplines, including data analysis, quantum mechanics, engineering, and computational science, making it a crucial concept in linear algebra with numerous practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f314a3",
   "metadata": {},
   "source": [
    "## Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the Eigen-Decomposition approach? Provide a brief proof to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f242a",
   "metadata": {},
   "source": [
    "A square matrix is diagonalizable using the Eigen-Decomposition approach if and only if it satisfies the following conditions:\n",
    "\n",
    "1. The matrix must be a square matrix, meaning it has the same number of rows and columns.\n",
    "\n",
    "2. The matrix must have a complete set of linearly independent eigenvectors. In other words, the matrix must have as many linearly independent eigenvectors as its dimension.\n",
    "\n",
    "Proof:\n",
    "\n",
    "Let A be a square matrix of dimension n x n. To determine whether A is diagonalizable, we need to find n linearly independent eigenvectors corresponding to A's eigenvalues.\n",
    "\n",
    "Suppose A has n distinct eigenvalues (λ₁, λ₂, ..., λₙ) with corresponding eigenvectors (v₁, v₂, ..., vₙ). We need to show that these eigenvectors are linearly independent.\n",
    "\n",
    "Assume that there exists a linear combination of the eigenvectors equal to the zero vector:\n",
    "\n",
    "c₁v₁ + c₂v₂ + ... + cₙvₙ = 0,\n",
    "\n",
    "where c₁, c₂, ..., cₙ are constants, and v₁, v₂, ..., vₙ are the eigenvectors.\n",
    "\n",
    "Now, we can pre-multiply both sides of this equation by A:\n",
    "\n",
    "A(c₁v₁ + c₂v₂ + ... + cₙvₙ) = A(0),\n",
    "\n",
    "Since A(vᵢ) = λᵢvᵢ for each i, we can rewrite the equation as:\n",
    "\n",
    "c₁λ₁v₁ + c₂λ₂v₂ + ... + cₙλₙvₙ = 0.\n",
    "\n",
    "Now, let's isolate λ₁v₁ on one side of the equation:\n",
    "\n",
    "c₁λ₁v₁ = -c₂λ₂v₂ - ... - cₙλₙvₙ.\n",
    "\n",
    "Since λ₁ is not equal to λ₂, λ₃, ..., or λₙ (by assumption that eigenvalues are distinct), c₁λ₁ cannot be canceled out by any other term on the right side. Therefore, c₁λ₁v₁ must be nonzero.\n",
    "\n",
    "However, the left side of the equation is zero (0) because c₁v₁ is a linear combination of eigenvectors, and they sum to the zero vector.\n",
    "\n",
    "This leads to a contradiction: we have a nonzero scalar (c₁λ₁) times the zero vector (0), which is impossible.\n",
    "\n",
    "Therefore, our initial assumption that there exists a nontrivial linear combination of the eigenvectors that equals zero is false. Hence, the eigenvectors (v₁, v₂, ..., vₙ) are linearly independent.\n",
    "\n",
    "Since A has n linearly independent eigenvectors, it is diagonalizable. This concludes the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1053bb3e",
   "metadata": {},
   "source": [
    "## Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach? How is it related to the diagonalizability of a matrix? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff29443",
   "metadata": {},
   "source": [
    "The Spectral Theorem is a fundamental result in linear algebra that plays a significant role in the context of the Eigen-Decomposition approach. It is closely related to the diagonalizability of a matrix and provides conditions under which a matrix can be diagonalized.\n",
    "\n",
    "**Significance of the Spectral Theorem:**\n",
    "\n",
    "1. **Matrix Diagonalization:** The Spectral Theorem states that for certain classes of matrices, particularly symmetric and Hermitian matrices (real symmetric or complex Hermitian), there exists an orthogonal (or unitary in the complex case) matrix P and a diagonal matrix D such that A = PDP⁻¹ (or A = PDΛP⁻¹ in the complex case). This means that the original matrix A can be transformed into a diagonal matrix D by a similarity transformation involving P. This diagonalization simplifies many matrix operations and is a fundamental concept in linear algebra.\n",
    "\n",
    "2. **Eigenvalues and Eigenvectors:** The diagonal matrix D contains the eigenvalues of the original matrix A, and the columns of the orthogonal matrix P (or unitary matrix in the complex case) are the corresponding eigenvectors. This decomposition reveals essential information about the matrix, including the eigenvalues and their corresponding eigenvectors.\n",
    "\n",
    "**Example:**\n",
    "Let's consider a real symmetric matrix A:\n",
    "\n",
    "A = | 4 &nbsp;&nbsp;  2 |\n",
    "<br>&nbsp; &nbsp; &nbsp;&nbsp;    | 2 &nbsp;&nbsp; 5 |\n",
    "\n",
    "The Spectral Theorem tells us that this matrix can be diagonalized as:\n",
    "\n",
    "A = PDP⁻¹\n",
    "\n",
    "Where P is an orthogonal matrix, D is a diagonal matrix, and P⁻¹ is the inverse of P.\n",
    "\n",
    "By finding the eigenvalues and eigenvectors of A, we can obtain the following results:\n",
    "\n",
    "Eigenvalues of A:\n",
    "- λ₁ = 3\n",
    "- λ₂ = 6\n",
    "\n",
    "Eigenvectors of A:\n",
    "- v₁ = [1, -1]\n",
    "- v₂ = [1, 2]\n",
    "\n",
    "Now, we can construct the orthogonal matrix P using the normalized eigenvectors:\n",
    "\n",
    "P = [v₁/‖v₁‖, v₂/‖v₂‖]\n",
    "\n",
    "where ‖v₁‖ and ‖v₂‖ are the norms of the eigenvectors. P will be an orthogonal matrix since the eigenvectors are orthogonal.\n",
    "\n",
    "And, we can create the diagonal matrix D with the eigenvalues on the diagonal:\n",
    "\n",
    "D = | λ₁ &nbsp;&nbsp;  0 |\n",
    "<br>&nbsp; &nbsp; &nbsp;&nbsp; | 0  &nbsp;&nbsp;  λ₂ |\n",
    "\n",
    "Now, we can perform the similarity transformation:\n",
    "\n",
    "A = PDP⁻¹\n",
    "\n",
    "This transformation allows us to express A as a diagonal matrix D with the eigenvalues on the diagonal, revealing the spectral information of the matrix.\n",
    "\n",
    "In summary, the Spectral Theorem is significant because it guarantees the diagonalizability of certain classes of matrices, simplifying matrix operations and providing insights into the matrix's eigenvalues and eigenvectors. This theorem is a fundamental concept in linear algebra and has various applications in fields like quantum mechanics, signal processing, and data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b9a18",
   "metadata": {},
   "source": [
    "## Q5. How do you find the eigenvalues of a matrix and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1eda73",
   "metadata": {},
   "source": [
    "To find the eigenvalues of a matrix, you need to solve the characteristic equation associated with the matrix. Eigenvalues represent the scaling factors by which a matrix stretches or compresses space along its principal axes. \n",
    "\n",
    "**Step 1: Characteristic Equation**\n",
    "\n",
    "Given a square matrix A, you start by finding its eigenvalues (λ) by solving the characteristic equation:\n",
    "\n",
    "det(A - λI) = 0\n",
    "\n",
    "Where:\n",
    "- A is the matrix for which you want to find eigenvalues.\n",
    "- λ (lambda) is the eigenvalue you're solving for.\n",
    "- I is the identity matrix of the same size as A.\n",
    "- det() denotes the determinant of the matrix.\n",
    "\n",
    "**Step 2: Solve for λ**\n",
    "\n",
    "You solve the characteristic equation for λ. The values of λ that satisfy the equation are the eigenvalues of the matrix A.\n",
    "\n",
    "**Step 3: Interpretation**\n",
    "\n",
    "Eigenvalues represent the scaling factors by which the matrix A transforms space along its principal axes (eigenvectors). Here's what different eigenvalues indicate:\n",
    "\n",
    "1. **Positive Eigenvalues (λ > 0):** These eigenvalues indicate that the matrix stretches space along the corresponding eigenvector direction. The larger the eigenvalue, the greater the stretching effect.\n",
    "\n",
    "2. **Negative Eigenvalues (λ < 0):** These eigenvalues indicate that the matrix compresses space along the corresponding eigenvector direction. The smaller the eigenvalue (in absolute value), the stronger the compression.\n",
    "\n",
    "3. **Zero Eigenvalues (λ = 0):** These eigenvalues indicate that the matrix collapses space along the corresponding eigenvector direction. The eigenvector direction becomes a \"null\" direction where space collapses to a lower dimension.\n",
    "\n",
    "In summary, eigenvalues represent how a matrix A transforms space along specific directions defined by its eigenvectors. Positive eigenvalues indicate stretching, negative eigenvalues indicate compression, and zero eigenvalues indicate a collapse of space along certain axes. Eigenvalues are fundamental in various applications, including Principal Component Analysis (PCA), linear transformations, and differential equations in physics and engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2527ea49",
   "metadata": {},
   "source": [
    "## Q6. What are eigenvectors and how are they related to eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15baa66",
   "metadata": {},
   "source": [
    "Eigenvectors are non-zero vectors that remain in the same direction after being transformed by a square matrix. They are closely related to eigenvalues and are used in the context of eigen-decomposition.\n",
    "\n",
    "Mathematically, if you have a square matrix A and an eigenvector v, the relationship between them is given by:\n",
    "\n",
    "A * v = λ * v\n",
    "\n",
    "Where:\n",
    "- A is the square matrix.\n",
    "- v is the eigenvector.\n",
    "- λ (lambda) is the corresponding eigenvalue.\n",
    "\n",
    "In this equation, A * v represents the result of multiplying the matrix A by the eigenvector v, and λ * v represents the same vector scaled by the eigenvalue λ. \n",
    "\n",
    "Eigenvectors are characterized by the property that they only change in magnitude (scale) when multiplied by the matrix A, not in direction. In other words, they point in the same direction before and after the transformation by A.\n",
    "\n",
    "Eigenvectors are often used to describe the principal directions or axes of a matrix. When a matrix is diagonalized through eigen-decomposition, the eigenvectors become the columns of the transformation matrix Q, and the eigenvalues become the diagonal entries of the diagonal matrix Λ. This diagonalization simplifies matrix operations and is fundamental in various applications, including Principal Component Analysis (PCA) and solving systems of linear differential equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949f62e",
   "metadata": {},
   "source": [
    "## Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10cb9dc",
   "metadata": {},
   "source": [
    "The geometric interpretation of eigenvectors and eigenvalues :\n",
    "\n",
    "Imagine a 2D or 3D space, and within that space, you have a matrix that represents a transformation. Eigenvectors are like the fundamental axes of this space. When you apply the transformation (matrix) to these eigenvectors, they simply stretch or shrink but maintain their orientation. The eigenvalues indicate how much stretching or shrinking occurs along each eigenvector.\n",
    "\n",
    "In 2D, think of a matrix as a stretching and rotating operation. Eigenvectors are the axes of stretching or rotation, and eigenvalues tell you how much stretching happens along those axes.\n",
    "\n",
    "In 3D, it becomes more complex, but the idea is similar. Eigenvectors represent the primary directions of change, and eigenvalues quantify how much the space stretches or compresses along those directions.\n",
    "\n",
    "This geometric interpretation is fundamental in various fields, including linear algebra, data analysis, and computer graphics, as it helps us understand the impact of linear transformations on space and data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2af9c",
   "metadata": {},
   "source": [
    "## Q8. What are some real-world applications of eigen decomposition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf0ec7e",
   "metadata": {},
   "source": [
    "Eigen decomposition, also known as eigendecomposition, plays a crucial role in various real-world applications across different domains. Here are some notable examples:\n",
    "\n",
    "1. **Principal Component Analysis (PCA):** In data science and dimensionality reduction, eigendecomposition is used in PCA to find the principal components (eigenvectors) of a dataset. PCA is widely applied in fields like image compression, face recognition, and data visualization.\n",
    "\n",
    "2. **Quantum Mechanics:** In quantum mechanics, eigendecomposition is fundamental. Wave functions, representing quantum states, are expressed as linear combinations of eigenvectors of Hermitian operators, and the corresponding eigenvalues provide information about the energy levels and probabilities of quantum states.\n",
    "\n",
    "3. **Structural Engineering:** Eigendecomposition is used in structural analysis to determine the natural frequencies and mode shapes of structures like bridges and buildings. Understanding these modes helps engineers design and evaluate structures for safety and stability.\n",
    "\n",
    "4. **Image Processing:** In image processing, eigendecomposition is employed in techniques like the Karhunen-Loève Transform (KLT) to represent images efficiently and reduce noise while preserving important features.\n",
    "\n",
    "5. **Recommendation Systems:** Eigendecomposition and matrix factorization techniques are used in recommendation systems, such as collaborative filtering methods, to make personalized recommendations based on user-item interaction data.\n",
    "\n",
    "6. **Stability Analysis:** In control theory, eigendecomposition is used to analyze the stability of dynamic systems. Eigenvalues of the system's state matrix provide insights into whether the system is stable, marginally stable, or unstable.\n",
    "\n",
    "7. **Quantum Chemistry:** In computational chemistry, eigendecomposition is used to solve the Schrödinger equation and determine the electronic structure of molecules. Eigenvalues and eigenvectors represent energy levels and molecular orbitals.\n",
    "\n",
    "8. **Finance:** In finance, eigendecomposition is applied in portfolio optimization and risk assessment. It helps identify the principal factors driving asset returns and construct efficient portfolios.\n",
    "\n",
    "9. **Machine Learning:** Eigendecomposition is used in machine learning algorithms like Singular Value Decomposition (SVD) for various tasks, including collaborative filtering, data compression, and latent semantic analysis.\n",
    "\n",
    "10. **Computer Graphics:** In computer graphics, eigendecomposition is used for shape analysis, animation, and deformation modeling. It helps capture and manipulate the geometry of 3D objects.\n",
    "\n",
    "11. **Seismic Analysis:** In seismology and earthquake engineering, eigendecomposition is used to study seismic wave propagation, analyze ground motion records, and assess the behavior of structures during earthquakes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc1528d",
   "metadata": {},
   "source": [
    "## Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dce81b",
   "metadata": {},
   "source": [
    "No, a square matrix can have only one set of eigenvalues, but it can have multiple linearly independent eigenvectors corresponding to each eigenvalue. Each eigenvalue corresponds to a distinct eigenvector, and there can be as many eigenvectors as the multiplicity of the eigenvalue (i.e., how many times that eigenvalue appears as a root of the characteristic polynomial).\n",
    "\n",
    "- Each eigenvalue has its set of linearly independent eigenvectors.\n",
    "- Multiple eigenvalues can share some or all of their eigenvectors if they have the same multiplicity.\n",
    "\n",
    "Eigenvalues and eigenvectors are unique to a matrix and are essential in various mathematical and computational applications, such as diagonalization, Principal Component Analysis (PCA), and solving linear systems of equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db342f98",
   "metadata": {},
   "source": [
    "## Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning? Discuss at least three specific applications or techniques that rely on Eigen-Decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7e7c01",
   "metadata": {},
   "source": [
    "The Eigen-Decomposition approach is highly useful in data analysis and machine learning, providing valuable insights and techniques in various applications.\n",
    "\n",
    "1. **Principal Component Analysis (PCA):** PCA is a dimensionality reduction technique that leverages Eigen-Decomposition to identify the principal components (eigenvectors) and their corresponding importance (eigenvalues) in high-dimensional data. It is widely used for feature selection and visualization. By transforming data into a new coordinate system defined by the principal components, PCA helps reduce data complexity while retaining as much variance as possible. This simplifies data analysis, visualization, and modeling, making it an essential tool in fields like image processing, genetics, and natural language processing.\n",
    "\n",
    "2. **Spectral Clustering:** Spectral clustering is a powerful technique for clustering data based on graph theory. It involves constructing a similarity graph and then using Eigen-Decomposition on the graph Laplacian matrix to identify clusters. The eigenvectors associated with the smallest eigenvalues reveal the cluster structure of the data. Spectral clustering has applications in image segmentation, community detection in social networks, and natural language processing for document clustering.\n",
    "\n",
    "3. **Kernel Methods:** Eigen-Decomposition plays a crucial role in kernel methods, such as Support Vector Machines (SVMs) and kernel PCA. These methods involve transforming data into a higher-dimensional space using a kernel function and then applying Eigen-Decomposition to operate in this new space. SVMs use Eigen-Decomposition to find the support vectors, which are the critical data points for classification. Kernel PCA extends PCA to nonlinear dimensionality reduction by computing the principal components in the kernel-induced feature space. Kernel methods are widely used in classification, regression, and anomaly detection in machine learning.\n",
    "\n",
    "4. **Eigenfaces in Face Recognition:** Eigenfaces is an application of PCA in the domain of facial recognition. It represents faces as linear combinations of eigenvectors obtained from a training set of face images. By projecting an input face onto the eigenfaces, the algorithm can recognize faces in new images, making it a foundational technique in biometrics and security.\n",
    "\n",
    "5. **Quantum Mechanics:** Eigen-Decomposition is a fundamental concept in quantum mechanics. In quantum physics, observables like position, momentum, and spin are represented by Hermitian operators, and finding their eigenvalues and eigenvectors provides information about possible measurement outcomes and states of quantum systems. Eigen-Decomposition is essential for solving the Schrödinger equation and understanding quantum phenomena.\n",
    "\n",
    "\n",
    "The Eigen-Decomposition approach has broad applications in data analysis and machine learning, including dimensionality reduction, clustering, and kernel methods. It enables the extraction of meaningful patterns, features, and structures from complex data, facilitating more effective data-driven decision-making and modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
