{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39606ef8",
   "metadata": {},
   "source": [
    "## Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacfd383",
   "metadata": {},
   "source": [
    "ANOVA (Analysis of Variance) is a statistical method used to compare the means of three or more groups or treatments to determine if there are any significant differences among them. \n",
    "\n",
    "To use ANOVA and ensure the validity of its results, certain assumptions need to be met. These assumptions are :\n",
    "\n",
    "**Independence** : The observations within each group or treatment are independent of each other. This means that the values in one group should not be influenced or related to the values in another group.\n",
    "\n",
    "**Normality** : The data in each group should follow a normal distribution. The normality assumption is particularly important when the sample sizes are small, as ANOVA tends to be robust to violations when the sample sizes are large.\n",
    "\n",
    "**Homogeneity of Variance (Homoscedasticity)** : The variability of the data (variance) within each group should be roughly equal. In other words, the spread of the data points around the group means should be similar across all groups.\n",
    "\n",
    "**Absence of outliers** : There must no outliers in the data points\n",
    "\n",
    "\n",
    "Examples of Violations:\n",
    "\n",
    "**Violation of Independence**: In some experimental designs, the independence assumption may be violated if there is a hierarchical or nested structure in the data. For example, if you measure the performance of students within different classrooms, the students within the same classroom may not be independent of each other due to shared characteristics or teaching styles.\n",
    "\n",
    "**Violation of Normality**: If the data in any of the groups deviates significantly from a normal distribution, it can impact the validity of ANOVA results. For instance, if the data is strongly skewed or has heavy tails, the normality assumption may not hold.\n",
    "\n",
    "**Violation of Homoscedasticity**: Unequal variances among groups can lead to biased ANOVA results. For example, if the variability of test scores in one group is much larger than that in another group, the assumption of homogeneity of variance may not be met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f7bf42",
   "metadata": {},
   "source": [
    "## Q2. What are the three types of ANOVA, and in what situations would each be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7a30a",
   "metadata": {},
   "source": [
    "The three types of ANOVA (Analysis of Variance) are:\n",
    "\n",
    "**One-Way ANOVA**:\n",
    "\n",
    "<br>One-Way ANOVA is used when there is one categorical independent variable (also known as a factor) with three or more levels or groups, and we want to compare the means of a continuous dependent variable across these groups.\n",
    "<br>It is suitable for situations where we have one factor and want to determine if there are any significant differences in the means of the dependent variable across the different levels of that factor.\n",
    "<br>Example: Suppose we want to compare the average test scores of students in three different teaching methods (A, B, and C) to see if there is a significant difference in performance.\n",
    "\n",
    "**Two-Way ANOVA**:\n",
    "\n",
    "<br>Two-Way ANOVA is used when there are two categorical independent variables (factors), and we want to examine the interaction between these two factors and their effects on a continuous dependent variable.\n",
    "<br>It is suitable for situations where we have two factors, and we want to investigate how the means of the dependent variable vary across the combinations of levels of both factors.\n",
    "<br>Example: Suppose we want to analyze the effect of two factors, gender (male and female) and teaching method (A, B, and C), on the performance of students in a test.\n",
    "\n",
    "**Three-Way ANOVA**:\n",
    "\n",
    "<br>Three-Way ANOVA is an extension of the two-way ANOVA and is used when there are three categorical independent variables (factors).\n",
    "<br>It is suitable for situations where we have three factors, and we want to examine their individual effects and their interactions on a continuous dependent variable.\n",
    "<br>Example: Suppose we want to study the effects of three factors: age group (young, middle-aged, and old), treatment type (A, B, and C), and location (urban and rural) on the response time of participants in a cognitive test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aad82a",
   "metadata": {},
   "source": [
    "## Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb7e79",
   "metadata": {},
   "source": [
    "Partitioning of variance in ANOVA refers to the division of the total variability in the data into different components that are associated with various sources of variation.\n",
    "\n",
    "In ANOVA, the total variance in the data is broken down into three main components:\n",
    "\n",
    "**Between-Group Variance (Between-Treatments Variance)**:\n",
    "This component represents the variability in the dependent variable that can be attributed to the differences between the groups or treatments (levels of the independent variable).\n",
    "It measures the effect of the factors (independent variables) on the dependent variable. A large between-group variance indicates that the groups have different means, suggesting that the factors have a significant effect on the outcome.\n",
    "\n",
    "**Within-Group Variance (Within-Treatments Variance or Residual Variance)**:\n",
    "This component represents the variability in the dependent variable that cannot be explained by the differences between the groups. It accounts for the random or unexplained variation within each group.\n",
    "It measures the variability of data points within each group around the group mean. A large within-group variance suggests that there is considerable variability within the groups, making it harder to detect significant differences between the group means.\n",
    "\n",
    "**Total Variance**:\n",
    "This is the overall variability in the data, and it is the sum of the between-group variance and the within-group variance.\n",
    "It represents the total variation in the dependent variable across all groups and treatments.\n",
    "\n",
    "\n",
    "\n",
    "The importance of understanding the partitioning of variance in ANOVA lies in its ability to provide valuable insights into the significance and effects of the factors being studied. By breaking down the total variance into these components, ANOVA helps researchers assess the proportion of variability in the dependent variable that can be attributed to the factors of interest. This allows us to determine whether there are statistically significant differences between the group means and whether the factors have a significant impact on the outcome.\n",
    "\n",
    "In summary, understanding the partitioning of variance in ANOVA helps researchers draw meaningful conclusions about the effects of different factors on the dependent variable, making it a crucial concept in analyzing and interpreting ANOVA results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19985328",
   "metadata": {},
   "source": [
    "## Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c9382",
   "metadata": {},
   "source": [
    "Calculate the Total Sum of Squares (SST):\n",
    "<br>SST represents the total variability in the dependent variable.  \n",
    "<br>The sum of squared differences between individual data points (yi) and the mean of the response variable (y).\n",
    "\n",
    "SST = Σ(yi – y)2\n",
    "\n",
    "\n",
    "Calculate the Explained Sum of Squares (SSE):\n",
    "<br>SSE represents the variability in the dependent variable that can be attributed to the differences between the group means (treatments). \n",
    "<br>The sum of squared differences between predicted data points (ŷi) and observed data points (yi).\n",
    "\n",
    "SSE = Σ(ŷi – yi)2\n",
    "\n",
    "Calculate the Residual Sum of Squares (SSR):\n",
    "<br>SSR represents the unexplained variability in the dependent variable within each group. \n",
    "<br>The sum of squared differences between predicted data points (ŷi) and the mean of the response variable(y).\n",
    "\n",
    "SSR = Σ(ŷi – y)2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a32775d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hours</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hours  score\n",
       "0      1     68\n",
       "1      1     76\n",
       "2      1     74\n",
       "3      2     80\n",
       "4      2     76"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the Data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#create pandas DataFrame\n",
    "df = pd.DataFrame({'hours': [1, 1, 1, 2, 2, 2, 2, 2, 3, 3,\n",
    "                             3, 4, 4, 4, 5, 5, 6, 7, 7, 8],\n",
    "                   'score': [68, 76, 74, 80, 76, 78, 81, 84, 86, 83,\n",
    "                             88, 85, 89, 94, 93, 94, 96, 89, 92, 97]})\n",
    "\n",
    "#view first five rows of DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6b49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a regression model\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#define response variable\n",
    "y = df['score']\n",
    "\n",
    "#define predictor variable\n",
    "x = df[['hours']]\n",
    "\n",
    "#add constant to predictor variables\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "#fit linear regression model\n",
    "model = sm.OLS(y, x).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4261689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSE :  331.0748847926266\n",
      "SSR :  917.4751152073719\n",
      "SST :  1248.5499999999986\n"
     ]
    }
   ],
   "source": [
    "#Calculate SSE,SSR,SST\n",
    "import numpy as np\n",
    "\n",
    "#calculate sse\n",
    "sse = np.sum((model.fittedvalues - df.score)**2)\n",
    "print(\"SSE : \", sse)\n",
    "\n",
    "\n",
    "#calculate ssr\n",
    "ssr = np.sum((model.fittedvalues - df.score.mean())**2)\n",
    "print(\"SSR : \", ssr)\n",
    "\n",
    "\n",
    "#calculate sst\n",
    "sst = ssr + sse\n",
    "print(\"SST : \", sst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797c2c3",
   "metadata": {},
   "source": [
    "## Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce1f179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             df     sum_sq   mean_sq         F    PR(>F)\n",
      "C(Fertilizer)               1.0   0.033333  0.033333  0.012069  0.913305\n",
      "C(Watering)                 1.0   1.421614  1.421614  0.514722  0.479045\n",
      "C(Fertilizer):C(Watering)   1.0   0.223698  0.223698  0.080994  0.778049\n",
      "Residual                   28.0  77.333333  2.761905       NaN       NaN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>height</td>      <th>  R-squared:         </th> <td>   0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td> 0.01207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 30 Jul 2023</td> <th>  Prob (F-statistic):</th>  <td> 0.913</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:42:56</td>     <th>  Log-Likelihood:    </th> <td> -56.772</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    30</td>      <th>  AIC:               </th> <td>   117.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    28</td>      <th>  BIC:               </th> <td>   120.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                        <td></td>                           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                     <td>   14.8000</td> <td>    0.429</td> <td>   34.491</td> <td> 0.000</td> <td>   13.921</td> <td>   15.679</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Fertilizer)[T.weekly]</th>                       <td>   -0.0222</td> <td>    0.202</td> <td>   -0.110</td> <td> 0.913</td> <td>   -0.437</td> <td>    0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Watering)[T.weekly]</th>                         <td>   -0.0222</td> <td>    0.202</td> <td>   -0.110</td> <td> 0.913</td> <td>   -0.437</td> <td>    0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Fertilizer)[T.weekly]:C(Watering)[T.weekly]</th> <td>   -0.0222</td> <td>    0.202</td> <td>   -0.110</td> <td> 0.913</td> <td>   -0.437</td> <td>    0.392</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.177</td> <th>  Durbin-Watson:     </th> <td>   0.916</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.915</td> <th>  Jarque-Bera (JB):  </th> <td>   0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.029</td> <th>  Prob(JB):          </th> <td>   0.995</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.929</td> <th>  Cond. No.          </th> <td>6.28e+31</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 1.64e-62. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 height   R-squared:                       0.000\n",
       "Model:                            OLS   Adj. R-squared:                 -0.035\n",
       "Method:                 Least Squares   F-statistic:                   0.01207\n",
       "Date:                Sun, 30 Jul 2023   Prob (F-statistic):              0.913\n",
       "Time:                        15:42:56   Log-Likelihood:                -56.772\n",
       "No. Observations:                  30   AIC:                             117.5\n",
       "Df Residuals:                      28   BIC:                             120.3\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=================================================================================================================\n",
       "                                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                        14.8000      0.429     34.491      0.000      13.921      15.679\n",
       "C(Fertilizer)[T.weekly]                          -0.0222      0.202     -0.110      0.913      -0.437       0.392\n",
       "C(Watering)[T.weekly]                            -0.0222      0.202     -0.110      0.913      -0.437       0.392\n",
       "C(Fertilizer)[T.weekly]:C(Watering)[T.weekly]    -0.0222      0.202     -0.110      0.913      -0.437       0.392\n",
       "==============================================================================\n",
       "Omnibus:                        0.177   Durbin-Watson:                   0.916\n",
       "Prob(Omnibus):                  0.915   Jarque-Bera (JB):                0.011\n",
       "Skew:                           0.029   Prob(JB):                        0.995\n",
       "Kurtosis:                       2.929   Cond. No.                     6.28e+31\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.64e-62. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "  \n",
    "# Create a dataframe\n",
    "dataframe = pd.DataFrame({'Fertilizer': np.repeat(['daily', 'weekly'], 15),\n",
    "                          'Watering': np.repeat(['daily', 'weekly'], 15),\n",
    "                          'height': [14, 16, 15, 15, 16, 13, 12, 11,\n",
    "                                     14, 15, 16, 16, 17, 18, 14, 13, \n",
    "                                     14, 14, 14, 15, 16, 16, 17, 18,\n",
    "                                     14, 13, 14, 14, 14, 15]})\n",
    "  \n",
    "  \n",
    "# Performing two-way ANOVA\n",
    "model = ols('height ~ C(Fertilizer) + C(Watering) + C(Fertilizer):C(Watering)',data=dataframe).fit()\n",
    "result = sm.stats.anova_lm(model, type=2)\n",
    "  \n",
    "# Print the result\n",
    "print(result)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc17f04",
   "metadata": {},
   "source": [
    "## Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would you interpret these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2f2e8",
   "metadata": {},
   "source": [
    "In a one-way ANOVA, the F-statistic is used to test whether there are significant differences between the means of three or more groups. The associated p-value indicates the probability of observing the data, or more extreme data, under the assumption that the group means are all equal (null hypothesis).\n",
    "\n",
    "The F-statistic:\n",
    "\n",
    "The F-statistic measures the ratio of variability between groups to variability within groups. A larger F-statistic suggests that the variability between group means is significantly greater than the variability within groups.\n",
    "\n",
    "The p-value:\n",
    "\n",
    "The p-value indicates the probability of obtaining the observed data or more extreme data under the assumption that there are no true differences between the group means (null hypothesis). A smaller p-value suggests that the observed differences are unlikely to have occurred by chance alone.\n",
    "Interpretation:\n",
    "\n",
    "Since the p-value (0.02) is less than the chosen significance level (α) of 0.05 (commonly used so assumption is made), we reject the null hypothesis.\n",
    "<br>This means that there is sufficient evidence to conclude that there are statistically significant differences between the means of the groups being compared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c72e244",
   "metadata": {},
   "source": [
    "## Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c0547e",
   "metadata": {},
   "source": [
    "In a repeated measures ANOVA, missing data can occur when participants have incomplete responses or when data is lost during data collection or processing. Handling missing data appropriately is crucial because it can impact the validity and reliability of the results. \n",
    "\n",
    "There are several methods to handle missing data in a repeated measures ANOVA:\n",
    "\n",
    "**Complete Case Analysis (Listwise Deletion)**: This method involves excluding any case with missing data in any of the variables being analyzed. While it is the simplest approach, it can lead to biased results if the data is not missing completely at random. It can also reduce the sample size and statistical power, potentially leading to less reliable results.\n",
    "\n",
    "**Mean Imputation**: In this method, missing data in a variable are replaced by the mean of that variable from the observed cases. While this is a straightforward approach, it may distort the distribution of the variable and underestimate the standard error, leading to overly optimistic statistical significance.\n",
    "\n",
    "**Last Observation Carried Forward (LOCF)**: LOCF imputes missing data with the value of the last observed data point. This method assumes that the data follows a linear pattern, which may not be appropriate for all situations.\n",
    "\n",
    "**Multiple Imputation**: Multiple imputation involves creating multiple plausible imputations for the missing data, incorporating uncertainty in the imputation process. This approach can provide more reliable estimates and standard errors. However, it can be computationally intensive and may require making assumptions about the missing data mechanism.\n",
    "\n",
    "**Maximum Likelihood Estimation (MLE)**: MLE is a statistical approach that estimates parameters by maximizing the likelihood function. In the context of missing data, it allows for the use of all available data and provides unbiased estimates under the assumption that data is missing at random.\n",
    "\n",
    "**Pattern-Mixture Models**: These models involve considering different patterns of missingness and fitting separate models for each pattern. This approach can be complex but may provide more accurate estimates when the missing data mechanism is related to the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e4db0",
   "metadata": {},
   "source": [
    "## Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca53a0e",
   "metadata": {},
   "source": [
    "Post-hoc tests are used in ANOVA to compare specific pairs of groups after a significant main effect or interaction effect has been found.\n",
    "\n",
    "Some common post-hoc tests include are :\n",
    "\n",
    "Tukey's Honestly Significant Difference (HSD): Tukey's HSD test is widely used when comparing all possible pairs of group means. It controls the familywise error rate, ensuring that the overall experimentwise error rate remains at a desired level (typically 0.05). This test is appropriate when you have equal group sizes and homogeneity of variances.\n",
    "\n",
    "Bonferroni Correction: The Bonferroni correction is a simple method to adjust the significance level for multiple comparisons. It divides the desired alpha level (usually 0.05) by the number of comparisons being made. This method is more conservative but can be applied to any set of comparisons.\n",
    "\n",
    "Sidak Correction: Similar to Bonferroni, the Sidak correction is another way to adjust the significance level for multiple comparisons. It is considered slightly more powerful than Bonferroni.\n",
    "\n",
    "Dunnett's Test: Dunnett's test is used when you have one control group and you want to compare it to multiple treatment groups. It controls the Type I error rate by considering the control group as a reference.\n",
    "\n",
    "Scheffé Test: The Scheffé test is more conservative than Tukey's HSD and is suitable when group sizes are unequal and variances are not homogeneous. It can be used for all possible pairwise comparisons.\n",
    "\n",
    "Fisher's Least Significant Difference (LSD): Fisher's LSD test is less conservative than Tukey's HSD, but it is not appropriate when there are unequal group sizes or non-homogeneous variances.\n",
    "\n",
    "The choice of post-hoc test depends on the research question, the number of groups, and the prior knowledge about which groups are likely to differ. A post-hoc test might be necessary when an ANOVA indicates a significant difference between groups but does not identify which specific groups differ. \n",
    "\n",
    "For example, a researcher might conduct an ANOVA to examine the effect of different instructional methods on student achievement. If the ANOVA shows a significant main effect of instructional method, the researcher might use a post-hoc test to compare the mean scores of each instructional method to identify which methods are significantly different from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073e51bc",
   "metadata": {},
   "source": [
    "## Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets. Report the F-statistic and p-value, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea66a198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 41.80444706032352\n",
      "p-value: 4.2309140010930765e-15\n",
      "We reject the null hypothesis.\n",
      "Final Conclusion : The mean weight loss is different for at least one diet.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Generate simulated data assuming normal distribution with same variance\n",
    "np.random.seed(20)\n",
    "diet_A = np.random.normal(5, 1, 50)\n",
    "diet_B = np.random.normal(4, 1, 50)\n",
    "diet_C = np.random.normal(3, 1, 50)\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Set significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Null hypothesis: The mean weight loss is the same for all three diets.\n",
    "# Alternative hypothesis: The mean weight loss is different for at least one diet.\n",
    "null_hypothesis = \"The mean weight loss is the same for all three diets.\"\n",
    "alternate_hypothesis = \"The mean weight loss is different for at least one diet.\"\n",
    "\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "if p_value < alpha:\n",
    "    print(\"We reject the null hypothesis.\")\n",
    "    print(f\"Final Conclusion : {alternate_hypothesis}\")\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis.\")\n",
    "    print(f\"Final Conclusion : {null_hypothesis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda96c94",
   "metadata": {},
   "source": [
    "## Q10. A company wants to know if there are any significant differences in the average time it takes to complete a task using three different software programs: Program A, Program B, and Program C. They randomly assign 30 employees to one of the programs and record the time it takes each employee to complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or interaction effects between the software programs and employee experience level (novice vs. experienced). Report the F-statistics and p-values, and interpret the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f03f6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  df       sum_sq    mean_sq         F  \\\n",
      "C(Software)                      2.0     9.309580   4.654790  0.216246   \n",
      "C(ExperienceLevel)               1.0    31.851905  31.851905  1.479736   \n",
      "C(Software):C(ExperienceLevel)   2.0    52.479686  26.239843  1.219018   \n",
      "Residual                        84.0  1808.132913  21.525392       NaN   \n",
      "\n",
      "                                  PR(>F)  \n",
      "C(Software)                     0.805984  \n",
      "C(ExperienceLevel)              0.227223  \n",
      "C(Software):C(ExperienceLevel)  0.300694  \n",
      "Residual                             NaN  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Generate random data for the example (replace this with your actual data)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Software programs: A, B, C\n",
    "software_programs = np.random.choice(['A', 'B', 'C'], size=90)\n",
    "\n",
    "# Employee experience level: Novice, Experienced\n",
    "experience_level = np.random.choice(['Novice', 'Experienced'], size=90)\n",
    "\n",
    "# Random time data for each combination of program and experience level\n",
    "time_to_complete_task = np.random.normal(loc=20, scale=5, size=90)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'Software': software_programs,\n",
    "                     'ExperienceLevel': experience_level,\n",
    "                     'Time': time_to_complete_task})\n",
    "\n",
    "# Perform the two-way ANOVA\n",
    "model = ols('Time ~ C(Software) + C(ExperienceLevel) + C(Software):C(ExperienceLevel)', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "\n",
    "# Report the results\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bedb6d",
   "metadata": {},
   "source": [
    "## Q11. An educational researcher is interested in whether a new teaching method improves student test scores. They randomly assign 100 students to either the control group (traditional teaching method) or the experimental group (new teaching method) and administer a test at the end of the semester. Conduct a two-sample t-test using Python to determine if there are any significant differences in test scores between the two groups. If the results are significant, follow up with a post-hoc test to determine which group(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4a32898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-sample t-test:\n",
      "t-statistic: -7.738786904885969\n",
      "p-value: 5.026085102727666e-13\n",
      "There is a significant difference in test scores between the control and experimental groups.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Generate random data for the example (replace this with your actual data)\n",
    "np.random.seed(42)\n",
    "\n",
    "control_group = np.random.normal(loc=75, scale=5, size=100)\n",
    "experimental_group = np.random.normal(loc=80, scale=6, size=100)\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_group, experimental_group)\n",
    "\n",
    "# Report the results\n",
    "print(\"Two-sample t-test:\")\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05  # Significance level\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant difference in test scores between the control and experimental groups.\")\n",
    "else:\n",
    "    print(\"There is no significant difference in test scores between the control and experimental groups.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83b708ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tukey's HSD post-hoc test:\n",
      "  Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n",
      "========================================================\n",
      " group1    group2    meandiff p-adj lower  upper  reject\n",
      "--------------------------------------------------------\n",
      "Control Experimental   5.6531   0.0 4.2125 7.0936   True\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Combine the test scores and group information into a DataFrame\n",
    "data = pd.DataFrame({'Test_Score': np.concatenate([control_group, experimental_group]),\n",
    "                     'Group': ['Control'] * 100 + ['Experimental'] * 100})\n",
    "\n",
    "# Perform Tukey's HSD post-hoc test\n",
    "tukey_results = pairwise_tukeyhsd(data['Test_Score'], data['Group'])\n",
    "\n",
    "# Report the results\n",
    "print(\"\\nTukey's HSD post-hoc test:\")\n",
    "print(tukey_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205fae3e",
   "metadata": {},
   "source": [
    "## Q12. A researcher wants to know if there are any significant differences in the average daily sales of three retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store on those days. Conduct a repeated measures ANOVA using Python to determine if there are any significant differences in sales between the three stores. If the results are significant, follow up with a post-hoc test to determine which store(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be14bec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way repeated measures ANOVA:\n",
      "F-statistic: 23.62763182315457\n",
      "p-value: 6.369054894762179e-09\n",
      "There is a significant difference in average daily sales between the three stores.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Generate random data for the example (replace this with your actual data)\n",
    "np.random.seed(42)\n",
    "\n",
    "store_A_sales = np.random.normal(loc=1000, scale=100, size=30)\n",
    "store_B_sales = np.random.normal(loc=950, scale=90, size=30)\n",
    "store_C_sales = np.random.normal(loc=1100, scale=110, size=30)\n",
    "\n",
    "# Combine the sales data and group information into a DataFrame\n",
    "data = pd.DataFrame({'Sales': np.concatenate([store_A_sales, store_B_sales, store_C_sales]),\n",
    "                     'Store': ['A'] * 30 + ['B'] * 30 + ['C'] * 30})\n",
    "\n",
    "# Perform one-way repeated measures ANOVA\n",
    "F_statistic, p_value = stats.f_oneway(store_A_sales, store_B_sales, store_C_sales)\n",
    "\n",
    "# Report the results\n",
    "print(\"One-way repeated measures ANOVA:\")\n",
    "print(\"F-statistic:\", F_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05  # Significance level\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant difference in average daily sales between the three stores.\")\n",
    "else:\n",
    "    print(\"There is no significant difference in average daily sales between the three stores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "194f8cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tukey's HSD post-hoc test:\n",
      "  Multiple Comparison of Means - Tukey HSD, FWER=0.05  \n",
      "=======================================================\n",
      "group1 group2 meandiff p-adj    lower    upper   reject\n",
      "-------------------------------------------------------\n",
      "     A      B -42.0899 0.2045 -100.5291  16.3492  False\n",
      "     A      C  120.232    0.0   61.7929 178.6712   True\n",
      "     B      C 162.3219    0.0  103.8828 220.7611   True\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Perform Tukey's HSD post-hoc test\n",
    "tukey_results = pairwise_tukeyhsd(data['Sales'], data['Store'])\n",
    "\n",
    "# Report the results\n",
    "print(\"\\nTukey's HSD post-hoc test:\")\n",
    "print(tukey_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
